{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5739ba48-5458-4877-b67d-935f0b276bac",
   "metadata": {},
   "source": [
    "# Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a0917-927e-479f-891e-14a1644b5945",
   "metadata": {},
   "source": [
    "### Attention is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b6a6cc-f625-4448-96cb-249f7b9cab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb189e9a-04a7-4602-87a7-6fc402d80e48",
   "metadata": {},
   "source": [
    "* The paper revolutionized NLP, leading to significant advancements in machine translation, text summarization, and other tasks.\n",
    "* The architechture enabled parallel processing of tokens, significantly improving training and inference speeds by utilizing the GPU resources fully.\n",
    "* Long sequences could be handled without losss of information.\n",
    "* The model contains three important blocks.\n",
    "    * Multihead Attention Module\n",
    "    * Position Embeddings\n",
    "    * Feed forwaard network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b110fb0-69ef-4085-af78-bc8ba988a371",
   "metadata": {},
   "source": [
    "#### Scaled Dot Product Attention\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}}) V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d6d330-3553-4a86-af1f-3c60df3bd3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: batch_size x n_heads x query_len x value_len\n",
    "# keys: batch_size x n_heads x query_len x value_len\n",
    "# value: batch_size x n_heads x query_len x value_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9ad0b76-934c-494f-aebb-73ee479c4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, norm,dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.norm = norm\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2,-1))\n",
    "        scores = scores / self.norm\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "        attention_probs = F.softmax(scores,dim=-1)\n",
    "        output = torch.matmul(self.dropout(attention_probs), value)\n",
    "        return output, attention_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1020ef0d-99c1-4616-a510-625437c11a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdp = ScaledDotProductAttention(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2eeb7741-fcef-4762-b01b-8991fc579134",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, attns = sdp.forward(torch.randn((64,8,256,512)),torch.randn((64,8,256,512)),torch.randn((64,8,256,512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5356e9fe-acef-41b5-88c0-85c4b0f62ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8, 256, 512])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "679b9186-d625-42bb-a8e2-1fca54f259e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8, 256, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9fc40-6490-4b44-8903-05081df7a5bd",
   "metadata": {},
   "source": [
    "#### Multihead Attention\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O \\\\\n",
    "\\text{where}\\ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\\n",
    "W_i^Q \\in \\mathbb{R}^{\\mathrm{d_{model}\\times d_k}}, W_i^K \\in \\mathbb{R}^{\\mathrm{d_{model}\\times d_k}}, W_i^V \\in \\mathbb{R}^{\\mathrm{d_{model}\\times d_v}}, W_i^O \\in \\mathbb{R}^{\\mathrm{hd_v\\times d_{model}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6ccf6a9b-5604-41ae-b435-0db4c0c65da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, n_heads,dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0, \"`d_model` should be a multiple of `n_heads`\"\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = int(d_model / n_heads)\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(np.sqrt(self.d_k))\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        q: batch_size x query_len x d_model\n",
    "        k: batch_size x query_len x d_model\n",
    "        v: batch_size x query_len x d_model\n",
    "        mask: batch_size x 1 x source_seq_len\n",
    "              batch_size x tgt_seq_len x tgt_seq_len\n",
    "        \"\"\"\n",
    "        print(q.size())\n",
    "\n",
    "        Q = q.view(q.size(0), -1, self.n_heads, self.d_k).transpose(1,2) # batch_size x n_heads x query_len x d_k\n",
    "        K = k.view(k.size(0), -1, self.n_heads, self.d_k).transpose(1,2)\n",
    "        V = v.view(v.size(0), -1, self.n_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        # calc attention\n",
    "        x, attn = self.attention(Q,K,V)\n",
    "\n",
    "        # regroup \n",
    "        x = x.transpose(1,2).contiguous().view(x.size(0), -1, self.n_heads * self.d_k)\n",
    "        x = self.W_o(x)\n",
    "\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c9cb9fe4-993d-4b5f-b0a5-6ff3963acf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(d_model=512,n_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "80f39c1e-035a-4a2e-b7ce-1b3bb87fee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 512])\n"
     ]
    }
   ],
   "source": [
    "output, attns = mha.forward(torch.randn((64,256,512)),torch.randn((64,256,512)),torch.randn((64,256,512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e843998d-0bc2-4dd9-8381-9a430a0b12bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 512])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1b1faa3a-3b39-4e25-9fc6-276d45621e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8, 256, 256])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attns.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64477c5-e717-4122-9e11-1b2cde922d3a",
   "metadata": {},
   "source": [
    "#### Feed forward Network\n",
    "\n",
    "$$\n",
    "FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e127584-2be5-43a8-a39f-3640680e4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout_rate=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.dropout(F.relu(self.w_1(x)))\n",
    "        x = self.w_2(x)\n",
    "        return x\n",
    "        # batch_size, seq_len, d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43216d-f89b-49b4-8b05-95090d8d2a5f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{PE}_{(pos, 2i)} &= \\text{sin}(\\frac{pos}{10000^{2i/d_{model}}}) \\\\\n",
    "\\text{PE}_{(pos, 2i + 1)} &= \\text{cos}(\\frac{pos}{10000^{2i/d_{model}}})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe826d5-9aec-4b3e-a5bc-42a644139b9c",
   "metadata": {},
   "source": [
    "#### Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9cb88d1a-3cba-4ff1-88ba-eb716bce6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout_rate=0.1,maxlen=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        pe = torch.zeros(maxlen, d_model)\n",
    "        position = torch.arange(0, maxlen).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )  # (d_model,)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # make these static\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        x= self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ca23dc9-47a5-4881-8ae7-83f7d0a1b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.attention_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
    "        self.attention_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.ff_layer= PositionFeedForward(d_model, d_ff, dropout_rate)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    def forward(self,x, mask):\n",
    "\n",
    "        x1 = self.attention_layer(x,x,x,mask)\n",
    "        x = self.attention_layer_norm(x+ self.dropout(x1))\n",
    "        x1= self.ff_layer(x)\n",
    "        x = self.ff_layer_norm(x+self.dropout(x1))\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5bb94854-0b47-4d91-b844-742f022e6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout_rate=0.1,maxlen=10000):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "\n",
    "        self.tok_embedding = nn.Linear(vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEncoding(d_model, dropout_rate=dropout_rate,maxlen=maxlen)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout_rate)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, dropout_rate=dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x : batch_size x seq_len\n",
    "        x = self.tok_embedding(x)\n",
    "        x = self.pos_embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "        # batch_size x seq_len x d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c5fb4336-df81-4be1-b7e2-a1d1cbdfd0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
    "        self.attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.ff_layer = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.encoder_attn_layer = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
    "        self.encoder_attn_layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, x, encoder_op, src_mask, tgt_mask):\n",
    "\n",
    "        x1 = self.attn_layer(x,x,x,tgt_mask)\n",
    "        x = self.attn_layer_norm(x + self.dropout(x1))\n",
    "        x1, attn = self.encoder_attn_layer(x, encoder_op, encoder_op, src_mask)\n",
    "        x = self.encoder_attn_layer(x+self.dropout(x1))\n",
    "\n",
    "        x1 = self.ff_layer(x)\n",
    "        x = self.ff_layer_norm(x+nn.Dropout(x1))\n",
    "\n",
    "        return x, attn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8e10b4d3-3229-4089-9387-fdde53c183dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout_rate=0.1,maxlen=10000):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "\n",
    "        self.tok_embedding = nn.Linear(vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEncoding(d_model, dropout_rate=dropout_rate,maxlen=maxlen)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout_rate)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, dropout_rate=dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        # x : batch_size x seq_len\n",
    "        x = self.tok_embedding(x)\n",
    "        x = self.pos_embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encodermask)\n",
    "        x = self.layer_norm(x, enc_output, src_mask, tgt_mask)\n",
    "        return x\n",
    "        # batch_size x seq_len x d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "57411bb3-e1ee-410e-bb5f-bfc8190495eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, linear_mapper):\n",
    "\n",
    "        self.encoder= encoder\n",
    "        self.decoder= decoder\n",
    "        self.linear_mapper = linear_mapper\n",
    "\n",
    "    def forward(self,src,tgt):\n",
    "        enc_output = self.encoder(src)\n",
    "        dec_output, attn = self.decoder(tgt, enc_output)\n",
    "        output = self.linear_mapper(dec_output)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "90c4fd1f-4b36-4ed8-b37b-2ee75e5339de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(LinearMapper, self).__init__()\n",
    "        self.affine_map = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.affine_map(x) \n",
    "        output = F.log_softmax(x, dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7262f118-d503-4fa4-b9ec-978f579b9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = nn.Transformer(d_model=512, nhead=8, \n",
    "                             num_encoder_layers=6, num_decoder_layers=6, \n",
    "                             dim_feedforward=2048, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9b1c7ca2-39aa-4d47-a7b5-27146159a5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d884053-a1a4-4ffd-b515-9f8c43cf7ec4",
   "metadata": {},
   "source": [
    "### Finetuning Decoder Only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "a64cf0b5-ebdc-46b7-a3a5-7029e22f47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"SuryaKrishna02/aya-telugu-poems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "495bab3b-a738-41ce-a217-24e75a26fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(x):\n",
    "    for each_s in x:\n",
    "        yield each_s['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "2a6331aa-9339-440e-9f9b-6aa3a5eb44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generate_data(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9316fad1-4a1f-46f6-9728-0aa801a70323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=1000, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"])\n",
    "\n",
    "tokenizer.train_from_iterator(g, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "482a8066-fdff-43f3-9d87-c5236e695625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size))\n",
    "        pe = torch.zeros(max_len, 1, embed_size)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch, len, d_model\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:x.size(0), :, :].to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240652ec-4372-47e1-a1e3-03a21a719769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "d996e965-48b9-4317-a969-684712725802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, \n",
    "                 dropout_rate=0.1,max_len=32):\n",
    "        \n",
    "        super(DecoderOnlyTransformer, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.position_embedding = PositionalEncoding(d_model, max_len)\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, \n",
    "                                                   nhead =n_heads, \n",
    "                                                   dim_feedforward=d_ff)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.tok_embedding(x)\n",
    "        x = self.position_embedding(x)\n",
    "        x = self.transformer_decoder(x,memory=x)\n",
    "        logits = self.fc_out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "7f76c898-0048-4837-a25d-c0f61f003d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embed_size = 512  \n",
    "num_heads = 8  \n",
    "num_layers = 6 \n",
    "hidden_dim = 2048  \n",
    "max_len = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "037b48b5-486f-440b-8385-878bf85f0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = generate_data(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "d5b157d3-7654-4690-848d-7305f18fed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(corpus, tokenizer, max_len=32):\n",
    "    tokenized_corpus = [tokenizer.encode(sentence).ids for sentence in corpus]\n",
    "    tokenized_corpus = [x[0:max_len] for x in tokenized_corpus]\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "6adb6780-2125-4ef8-a60b-99849e205d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenize_data(g, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "075815b2-d973-4e2c-a210-d4da7b8a79c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5115"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "309a8404-76d2-4631-905d-f0454f445fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sequence_length = 32\n",
    "input_sequences = torch.zeros((batch_size, sequence_length), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "ba784c62-f71d-462e-a6d9-fbf6bf3948a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "a54c8079-e0b2-4edb-a8f2-33997c75a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(batch_size):\n",
    "    input_sequences[i, :len(tokenized_corpus[i])] = torch.tensor(tokenized_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "a41b789f-1b6d-42af-8270-5e19ac6c3b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "22a0fecc-b266-4e9c-ba67-6be386acd118",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = DecoderOnlyTransformer( vocab_size=30000, d_model=512, n_layers=6, n_heads=8, \n",
    "                       d_ff=512, dropout_rate=0.1,max_len=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "8fb7f43d-0540-47b8-98bd-4f504145a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "177a9af7-c97d-401a-9761-3340f9f6e5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 10.540102005004883\n",
      "Epoch 2/10, Loss: 8.720990180969238\n",
      "Epoch 3/10, Loss: 7.641439437866211\n",
      "Epoch 4/10, Loss: 6.5575408935546875\n",
      "Epoch 5/10, Loss: 5.865268230438232\n",
      "Epoch 6/10, Loss: 5.230606555938721\n",
      "Epoch 7/10, Loss: 4.630501747131348\n",
      "Epoch 8/10, Loss: 4.067417144775391\n",
      "Epoch 9/10, Loss: 3.589400053024292\n",
      "Epoch 10/10, Loss: 3.262800455093384\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    transformer.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = transformer(input_sequences)\n",
    "    \n",
    "    # Shift input by 1 token to compute next token prediction loss\n",
    "    target_sequences = input_sequences[:, 1:]\n",
    "    logits = logits[:, :-1, :].reshape(-1, 30000)  # Reshape for loss computation\n",
    "    target_sequences = target_sequences.reshape(-1)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(logits, target_sequences)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4414136-5ad1-4b86-a4e8-e3783041e7f0",
   "metadata": {},
   "source": [
    "### Lora Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2018113d-c61b-4293-a277-74ff726c1af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandeep/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d98a3b-54c7-443a-8fa6-69325517ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  \n",
    "    lora_alpha=32,  \n",
    "    target_modules=[\"c_attn\"],  \n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\",  #\n",
    "    task_type=\"CAUSAL_LM\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2a272d-5923-4621-948e-384dbe26df03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandeep/anaconda3/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80485ba1-6f35-489f-98e1-595ef6315569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset (e.g., WikiText)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], return_tensors=\"pt\", padding=True, truncation=True,max_length=512)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e7d78e-cd14-47ef-8d5d-8f4a788986db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_prepare_labels(examples):\n",
    "    # Tokenize the input text\n",
    "    tokenized_inputs = tokenizer(examples['text'], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # GPT-2 uses the input as the label, so we clone the input_ids\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization and labeling function\n",
    "tokenized_dataset = dataset.map(tokenize_and_prepare_labels, batched=True, remove_columns='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f8f5d1-fa08-4d52-9b2b-35b7d996aa02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset['train'][1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a44adb4-f55d-48c5-a8b3-4c1fc557c36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbb424ad-964d-4e21-bc7d-a3d81ef72b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandeep/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  \n",
    "    num_train_epochs=3,  \n",
    "    logging_dir=\"./logs\",  \n",
    "    logging_steps=10,  \n",
    "    save_steps=500, \n",
    "    learning_rate=5e-5,  \n",
    "    evaluation_strategy=\"steps\", \n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size =1,\n",
    "    per_device_eval_batch_size =1\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9580bd-f50c-4a39-941b-f1b8ae462701",
   "metadata": {},
   "source": [
    "### BERT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d9a7bd-05f0-44fc-b70b-f07bc1ac43b7",
   "metadata": {},
   "source": [
    "* Language models trained in unsupervised manner to learn good contextualized representations.\n",
    "* Contains only stacks of encoder layers (Mutli-head attention, Feed-foward blocks, Token embeddings, Position Embeddings)\n",
    "* Training is done using two objectives.\n",
    "    * Masked Language Modelling: Randomly masks some tokens in the input sequence and tries to predict it.\n",
    "    * Next Sentence Prediction: Predicts if the sentence pair are consecutive in training. \n",
    "* Special Tokens:\n",
    "    * [CLS] token is used at the beginning of every sentence, it is assumed to contain all the information present in the sentence, it is useful in classfication tasks.\n",
    "    * [SEP] token is used to separate sentences from each other so that the model can undestand and learn it.\n",
    "    * [PAD] token is used for padding data. [UNK] is for unknown tokens.\n",
    "* BERT + Finetuning classfication head achieves a good performance and strong baseline in text classification tasks. Before jumping on to generative AI, it is better to validate performance using strong pretrained BERT model. The inference cost of this model will be cheaper and the model can be finetuned to your usecase.\n",
    "* BERT uses trainable position embeddings unlike the original transformers which use sin/cos fixed embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cdf5b5f3-8b01-48a2-9d83-b668f36df116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class BERT(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, \n",
    "                 dropout_rate=0.1,max_len=32):\n",
    "        super(BERT, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout_rate       \n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model, n_heads,\n",
    "                                                 d_ff, dropout_rate)\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer,num_layers=n_layers)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_embed = self.token_embedding(x)\n",
    "        positions = torch.arange(0, self.max_len, dtype=torch.long)\n",
    "        positions = positions.unsqueeze(0).expand_as(x)\n",
    "        pos_embed = self.pos_embedding(positions)\n",
    "        segment_embed = token_embed + pos_embed\n",
    "        x = self.decoder(segment_embed,memory=segment_embed)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63b111c6-5da1-4e97-879b-22c3b4db8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT(vocab_size=30000, d_model=512, n_layers=6, n_heads=8, \n",
    "                       d_ff=512, dropout_rate=0.1,max_len=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36b6e934-dd69-46b1-b5a5-08f78eae6fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weight_sizes(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'Total parameters: {total_params}\\n')\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        param_size = param.numel()\n",
    "        percentage = 100 * param_size / total_params\n",
    "        print(f'Layer: {name} | Size: {param.size()} | Number of parameters: {param_size} | Percentage: {percentage:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd991ea2-fe15-4b7f-8ee4-a4ca973524f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 49173808\n",
      "\n",
      "Layer: token_embedding.weight | Size: torch.Size([30000, 512]) | Number of parameters: 15360000 | Percentage: 31.24%\n",
      "Layer: pos_embedding.weight | Size: torch.Size([32, 512]) | Number of parameters: 16384 | Percentage: 0.03%\n",
      "Layer: decoder_layer.self_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder_layer.self_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder_layer.self_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder_layer.self_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder_layer.multihead_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder_layer.multihead_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder_layer.multihead_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder_layer.multihead_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder_layer.linear1.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder_layer.linear1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder_layer.linear2.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder_layer.linear2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder_layer.norm1.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder_layer.norm1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder_layer.norm2.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder_layer.norm2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder_layer.norm3.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder_layer.norm3.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.self_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.0.self_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.self_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.0.self_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.multihead_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.0.multihead_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.multihead_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.0.multihead_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.linear1.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.0.linear1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.linear2.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.0.linear2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.norm1.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.norm1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.norm2.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.norm2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.norm3.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.0.norm3.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.self_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.1.self_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.self_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.1.self_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.multihead_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.1.multihead_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.multihead_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.1.multihead_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.linear1.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.1.linear1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.linear2.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.1.linear2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.norm1.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.norm1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.norm2.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.norm2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.norm3.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.1.norm3.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.self_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.2.self_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.self_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.2.self_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.multihead_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.2.multihead_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.multihead_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.2.multihead_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.linear1.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.2.linear1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.linear2.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.2.linear2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.norm1.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.norm1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.norm2.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.norm2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.norm3.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.2.norm3.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.self_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.3.self_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.self_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.3.self_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.multihead_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.3.multihead_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.multihead_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.3.multihead_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.linear1.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.3.linear1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.linear2.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.3.linear2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.norm1.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.norm1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.norm2.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.norm2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.norm3.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.3.norm3.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.self_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.4.self_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.self_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.4.self_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.multihead_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.4.multihead_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.multihead_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.4.multihead_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.linear1.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.4.linear1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.linear2.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.4.linear2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.norm1.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.norm1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.norm2.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.norm2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.norm3.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.4.norm3.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.self_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.5.self_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.self_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.5.self_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.multihead_attn.in_proj_weight | Size: torch.Size([1536, 512]) | Number of parameters: 786432 | Percentage: 1.60%\n",
      "Layer: decoder.layers.5.multihead_attn.in_proj_bias | Size: torch.Size([1536]) | Number of parameters: 1536 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.multihead_attn.out_proj.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.5.multihead_attn.out_proj.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.linear1.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.5.linear1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.linear2.weight | Size: torch.Size([512, 512]) | Number of parameters: 262144 | Percentage: 0.53%\n",
      "Layer: decoder.layers.5.linear2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.norm1.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.norm1.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.norm2.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.norm2.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.norm3.weight | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: decoder.layers.5.norm3.bias | Size: torch.Size([512]) | Number of parameters: 512 | Percentage: 0.00%\n",
      "Layer: fc_out.weight | Size: torch.Size([30000, 512]) | Number of parameters: 15360000 | Percentage: 31.24%\n",
      "Layer: fc_out.bias | Size: torch.Size([30000]) | Number of parameters: 30000 | Percentage: 0.06%\n"
     ]
    }
   ],
   "source": [
    "print_weight_sizes(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73662bc8-9a6c-43f5-b44b-a9f911640240",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randint(0, 10000, (16,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "918b47b1-305a-4965-b732-67cfd91692f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 30000])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.forward(inp).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c32f4f-3457-47dd-89bb-a7ed83ab2912",
   "metadata": {},
   "source": [
    "### LLama Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334a233-14af-4c60-be1e-59a8f28163bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
